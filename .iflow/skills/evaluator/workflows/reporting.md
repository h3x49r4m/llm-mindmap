# Report Generation Workflow

## Overview

The reporting phase generates a comprehensive evaluation report that summarizes test results, discovered issues, quality metrics, and provides prioritized recommendations for improvements.

## Process

### 1. Compile Test Results

**Data Sources:**

| Source | Data | Location |
|--------|------|----------|
| Feature Checklist | Feature statuses, results | `.state/evaluation.md` |
| Issues List | Issue descriptions, severity, locations | `.state/evaluation.md` |
| Progress Stats | Tested, passed, failed, partial, skipped counts | `.state/evaluation.md` |
| Project Metadata | Project type, stack, root path | `.state/evaluation.md` |

**Compilation Steps:**

1. Read state file: `.state/evaluation.md`
2. Parse feature checklist
3. Extract issues list
4. Calculate progress statistics
5. Generate quality metrics

### 2. Calculate Quality Metrics

**Metric Formulas:**

```
Feature Completeness = (Total Features Tested / Total Features Found) Ã— 100

Test Coverage = (Features with Existing Tests / Total Features) Ã— 100

Reliability = (Fully Passing Features / Tested Features) Ã— 100

UX Assessment = (Subjective score based on partial/fail results)
  - Deduct points for usability issues
  - Deduct points for mobile responsiveness issues
  - Deduct points for poor error messages
  - Base score: 100%
```

**Quality Score Calculation:**

```
Overall Quality Score = (
  (Feature Completeness Ã— 0.3) +
  (Reliability Ã— 0.4) +
  (UX Assessment Ã— 0.2) +
  (Test Coverage Ã— 0.1)
)

Interpretation:
  90-100%: Excellent
  80-89%: Good
  70-79%: Needs Improvement
  60-69%: Fair
  <60%: Poor
```

### 3. Determine Overall Status

**Status Determination:**

| Conditions | Status | Emoji |
|------------|--------|-------|
| No critical issues AND reliability â‰¥ 80% | Excellent | âœ… |
| Critical issues = 0 AND reliability â‰¥ 70% | Good | âœ… |
| Critical issues = 0 AND reliability â‰¥ 60% | Needs Improvement | âš ï¸ |
| Critical issues > 0 OR reliability < 60% | Poor | âŒ |

**Status Examples:**

```
Critical: 0, High: 0, Reliability: 85% â†’ âœ… Excellent
Critical: 0, High: 2, Reliability: 75% â†’ âš ï¸ Needs Improvement
Critical: 1, High: 0, Reliability: 90% â†’ âŒ Poor (due to critical issue)
Critical: 0, High: 0, Reliability: 55% â†’ âŒ Poor (due to low reliability)
```

### 4. Organize Feature Results

**Group by Status:**

```markdown
FEATURE TEST RESULTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… PASSED (<count>/<total>)
  â€¢ Feature 1: <name>
  â€¢ Feature 2: <name>
  â€¢ Feature 3: <name>
  ...

âŒ FAILED (<count>/<total>)
  â€¢ Feature X: <name> - <reason>
  â€¢ Feature Y: <name> - <reason>
  ...

âš ï¸ PARTIAL (<count>/<total>)
  â€¢ Feature A: <name> - <reason>
  â€¢ Feature B: <name> - <reason>
  ...

â¬œ SKIPPED (<count>/<total>)
  â€¢ Feature M: <name> - <reason>
  â€¢ Feature N: <name> - <reason>
  ...
```

### 5. Categorize Issues by Severity

**Severity Groups:**

```markdown
ISSUES DISCOVERED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ”´ CRITICAL (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ  HIGH PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ¡ MEDIUM PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ¢ LOW PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...
```

### 6. Generate Recommendations

**Recommendation Prioritization:**

**Immediate (This Sprint):**
- All critical issues
- High-priority issues blocking users
- Security vulnerabilities

**Short-Term (Next Sprint):**
- High-priority usability issues
- Medium-priority issues affecting core features
- Performance improvements

**Long-Term (Future):**
- Medium-priority enhancements
- Low-priority cosmetic issues
- Documentation improvements
- Test coverage improvements

**Recommendation Format:**

```markdown
RECOMMENDATIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ¯ IMMEDIATE (This Sprint)
  1. <Priority issue> - <severity>
  2. <Priority issue> - <severity>

ğŸ“… SHORT-TERM (Next Sprint)
  1. <Issue> - <severity>
  2. <Issue> - <severity>

ğŸ”® LONG-TERM (Future)
  1. <Improvement>
  2. <Improvement>
```

### 7. Generate Full Report

**Complete Report Structure:**

```markdown
ğŸ“Š PROJECT EVALUATION REPORT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Project: <project-name>
Evaluated: <date>
Features Tested: <X>/<N> (<Y>%)

EXECUTIVE SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Overall Status: <emoji> <STATUS>
Critical Issues: <N>
High Priority Issues: <N>
Medium Priority Issues: <N>
Low Priority Issues: <N>

Quality Score: <score>%
Feature Completeness: <X>%
Reliability: <Y>%
UX Assessment: <Z>%
Test Coverage: <W>%

KEY FINDINGS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ <Finding 1>
â€¢ <Finding 2>
â€¢ <Finding 3>

PROJECT DETAILS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Type: <project-type>
Stack: <technologies>
Root: <project-root-path>

FEATURE TEST RESULTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… PASSED (<count>/<total>)
  â€¢ Feature 1: <name>
  â€¢ Feature 2: <name>
  ...

âŒ FAILED (<count>/<total>)
  â€¢ Feature X: <name> - <reason>
  â€¢ Feature Y: <name> - <reason>
  ...

âš ï¸ PARTIAL (<count>/<total>)
  â€¢ Feature A: <name> - <reason>
  â€¢ Feature B: <name> - <reason>
  ...

â¬œ SKIPPED (<count>/<total>)
  â€¢ Feature M: <name> - <reason>
  â€¢ Feature N: <name> - <reason>
  ...

ISSUES DISCOVERED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ CRITICAL (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ  HIGH PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ¡ MEDIUM PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

ğŸŸ¢ LOW PRIORITY (<count>)
  1. <Issue description> - <location>
  2. <Issue description> - <location>
  ...

QUALITY METRICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Feature Completeness: <X>% (<tested>/<total> features tested)
Test Coverage: <Y>% (existing tests cover <n>/<total> features)
Reliability: <Z>% (<passed>/<tested> features fully working)
UX Assessment: <W>% (based on partial/fail results)

Overall Quality Score: <score>%
Interpretation: <Excellent|Good|Needs Improvement|Fair|Poor>

TESTING STATISTICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Features: <N>
Tested: <X> (<Y>%)
Passed: <A> (<P>% of tested)
Failed: <B> (<Q>% of tested)
Partial: <C> (<R>% of tested)
Skipped: <D> (<S>% of total)

Issues Found: <total>
  Critical: <count>
  High Priority: <count>
  Medium Priority: <count>
  Low Priority: <count>

RECOMMENDATIONS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ IMMEDIATE (This Sprint)
  1. <Priority issue> - <severity>
  2. <Priority issue> - <severity>

ğŸ“… SHORT-TERM (Next Sprint)
  1. <Issue> - <severity>
  2. <Issue> - <severity>

ğŸ”® LONG-TERM (Future)
  1. <Improvement>
  2. <Improvement>

NEXT STEPS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Address critical issues immediately
2. Fix high-priority issues in next sprint
3. Improve test coverage for failed features
4. Consider using dev-team skill to fix issues
5. Re-run evaluation after fixes

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Report Generated: <timestamp>
Evaluator Skill v1.0.0
```

### 8. Save Report File

**Report File Location:**
```
PROJECT_ROOT/.state/evaluation-report.md
```

**Save Operation:**
- Use write_file tool (only allowed in .state/ directory)
- Include timestamp in report
- Overwrite existing report if present

### 9. Update State File

Update `.state/evaluation.md` with report generation timestamp:

```markdown
# Evaluation State

Project: <project-name>
Started: <timestamp>
Last Updated: <timestamp>
Report Generated: <timestamp>

## Metadata
Type: <project-type>
Stack: <technologies>

## Feature Checklist
[... existing checklist ...]

## Issues
[... existing issues ...]

## Progress
Total Features: <N>
Tested: <X> (<Y>%)
Passed: <A>
Failed: <B>
Partial: <C>
Skipped: <D>

## Report
Last Generated: <timestamp>
File: .state/evaluation-report.md
```

### 10. Display Report Summary

**Summary Display:**

```
ğŸ“Š EVALUATION REPORT GENERATED
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… Overall Status: <STATUS>
ğŸ“ˆ Quality Score: <score>%

ğŸ“‹ Test Results:
  â€¢ Passed: <count> (<percentage>%)
  â€¢ Failed: <count> (<percentage>%)
  â€¢ Partial: <count> (<percentage>%)
  â€¢ Skipped: <count> (<percentage>%)

ğŸ› Issues Found:
  â€¢ Critical: <count>
  â€¢ High Priority: <count>
  â€¢ Medium Priority: <count>
  â€¢ Low Priority: <count>

ğŸ“ Full Report: .state/evaluation-report.md
ğŸ“ State File: .state/evaluation.md

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ’¡ Tip: Say "evaluator list issues" to see all issues
ğŸ’¡ Tip: Say "evaluator show checklist" to view full status
ğŸ’¡ Tip: Consider using dev-team skill to fix discovered issues
```

## Report Customization

### Minimal Report

For quick summary:

```markdown
ğŸ“Š QUICK SUMMARY
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Status: <STATUS>
Passed: <count>/<total>
Issues: <count> critical, <count> high
```

### Detailed Report

For comprehensive analysis (default format shown above).

### Export Formats

**Markdown (Default):**
- Human-readable
- Git-friendly
- Easy to edit

**JSON (Future Enhancement):**
```json
{
  "project": "<name>",
  "date": "<timestamp>",
  "status": "<STATUS>",
  "features": {
    "total": <N>,
    "tested": <X>,
    "passed": <A>,
    "failed": <B>,
    "partial": <C>,
    "skipped": <D>
  },
  "issues": {
    "critical": <count>,
    "high": <count>,
    "medium": <count>,
    "low": <count>
  },
  "metrics": {
    "completeness": <X>,
    "reliability": <Y>,
    "ux_assessment": <Z>,
    "test_coverage": <W>,
    "overall_score": <score>
  },
  "recommendations": [...]
}
```

## Integration with Other Skills

### dev-team

After evaluation, use dev-team to fix issues:

```
User: dev-team build "Fix critical and high-priority issues from evaluation report"

Dev-Team will:
1. Read evaluation report
2. Prioritize critical issues
3. Create tasks for each issue
4. Implement fixes following TDD
5. Verify fixes with tests
```

### git-manage

Commit evaluation report:

```
User: /git-manage commit docs: add evaluation report

This commits the evaluation report to git for tracking
```

### tdd-enforce

Check if failed features have corresponding tests:

```
User: /tdd-enforce

This verifies TDD compliance and identifies missing tests
```

## Exit Conditions

**Success:**
- Report generated
- Report file saved
- State file updated
- Summary displayed

**Warning:**
- Low test coverage (< 50%)
- Many features skipped (> 30%)
- High issue count

**Failure:**
- State file not found
- Report file save failed
- Data parsing errors

## Best Practices

1. **Generate reports regularly**: Don't wait until all features tested
2. **Track trends**: Compare reports over time
3. **Share with team**: Include in standup/sprint review
4. **Prioritize fixes**: Focus on critical and high-priority issues
5. **Re-evaluate**: After fixes, run evaluation again

## Next Steps

After report generation:

1. Review recommendations
2. Prioritize issues
3. Use dev-team skill to fix issues
4. Re-run evaluation after fixes
5. Track improvement trends